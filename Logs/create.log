root@02119e61dba1:/SAFE_VOLUME/infra# bin/run.sh create

Initializing the backend...

Initializing provider plugins...
- Reusing previous version of hashicorp/azurerm from the dependency lock file
- Reusing previous version of hashicorp/random from the dependency lock file
- Reusing previous version of hashicorp/tls from the dependency lock file
- Reusing previous version of hashicorp/local from the dependency lock file
- Reusing previous version of hashicorp/null from the dependency lock file
- Using previously-installed hashicorp/azurerm v2.64.0
- Using previously-installed hashicorp/random v3.1.0
- Using previously-installed hashicorp/tls v3.1.0
- Using previously-installed hashicorp/local v2.1.0
- Using previously-installed hashicorp/null v3.1.0

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
Acquiring state lock. This may take a few moments...

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create
 <= read (data resources)

Terraform will perform the following actions:

  # data.azurerm_public_ip.pi_k8s will be read during apply
  # (config refers to values not yet known)
 <= data "azurerm_public_ip" "pi_k8s"  {
      + allocation_method       = (known after apply)
      + domain_name_label       = (known after apply)
      + fqdn                    = (known after apply)
      + id                      = (known after apply)
      + idle_timeout_in_minutes = (known after apply)
      + ip_address              = (known after apply)
      + ip_tags                 = (known after apply)
      + ip_version              = (known after apply)
      + location                = (known after apply)
      + name                    = "pi_k8s"
      + resource_group_name     = "RG_Kubernetes"
      + reverse_fqdn            = (known after apply)
      + sku                     = (known after apply)
      + zones                   = (known after apply)

      + timeouts {
          + read = (known after apply)
        }
    }

  # azurerm_key_vault.keyvault_k8s will be created
  + resource "azurerm_key_vault" "keyvault_k8s" {
      + access_policy              = (known after apply)
      + id                         = (known after apply)
      + location                   = "westeurope"
      + name                       = "keyvaultk8s"
      + resource_group_name        = "RG_Kubernetes"
      + sku_name                   = "standard"
      + soft_delete_enabled        = (known after apply)
      + soft_delete_retention_days = 7
      + tenant_id                  = "899789dc-202f-44b4-8472-a6d40f9eb440"
      + vault_uri                  = (known after apply)

      + network_acls {
          + bypass                     = (known after apply)
          + default_action             = (known after apply)
          + ip_rules                   = (known after apply)
          + virtual_network_subnet_ids = (known after apply)
        }
    }

  # azurerm_key_vault_access_policy.keyvault_k8s_ro will be created
  + resource "azurerm_key_vault_access_policy" "keyvault_k8s_ro" {
      + certificate_permissions = [
          + "get",
          + "getissuers",
          + "list",
          + "listissuers",
        ]
      + id                      = (known after apply)
      + key_permissions         = [
          + "get",
          + "list",
        ]
      + key_vault_id            = (known after apply)
      + object_id               = (known after apply)
      + secret_permissions      = [
          + "get",
          + "list",
        ]
      + storage_permissions     = [
          + "get",
          + "getsas",
          + "list",
          + "listsas",
        ]
      + tenant_id               = "899789dc-202f-44b4-8472-a6d40f9eb440"
    }

  # azurerm_key_vault_access_policy.keyvault_k8s_rw will be created
  + resource "azurerm_key_vault_access_policy" "keyvault_k8s_rw" {
      + certificate_permissions = [
          + "create",
          + "delete",
          + "deleteissuers",
          + "get",
          + "getissuers",
          + "import",
          + "list",
          + "listissuers",
          + "managecontacts",
          + "manageissuers",
          + "purge",
          + "recover",
          + "setissuers",
          + "update",
          + "backup",
          + "restore",
        ]
      + id                      = (known after apply)
      + key_permissions         = [
          + "backup",
          + "create",
          + "decrypt",
          + "delete",
          + "encrypt",
          + "get",
          + "import",
          + "list",
          + "purge",
          + "recover",
          + "restore",
          + "sign",
          + "unwrapKey",
          + "update",
          + "verify",
          + "wrapKey",
        ]
      + key_vault_id            = (known after apply)
      + object_id               = "af6b03e0-8970-4010-a204-ed0b33702065"
      + secret_permissions      = [
          + "backup",
          + "delete",
          + "get",
          + "list",
          + "purge",
          + "recover",
          + "restore",
          + "set",
        ]
      + storage_permissions     = [
          + "backup",
          + "delete",
          + "deletesas",
          + "get",
          + "getsas",
          + "list",
          + "listsas",
          + "purge",
          + "recover",
          + "regeneratekey",
          + "restore",
          + "set",
          + "setsas",
          + "update",
        ]
      + tenant_id               = "899789dc-202f-44b4-8472-a6d40f9eb440"
    }

  # azurerm_key_vault_secret.mysql_admin_password will be created
  + resource "azurerm_key_vault_secret" "mysql_admin_password" {
      + id             = (known after apply)
      + key_vault_id   = (known after apply)
      + name           = "mysql-admin-password"
      + value          = (sensitive value)
      + version        = (known after apply)
      + versionless_id = (known after apply)
    }

  # azurerm_key_vault_secret.mysql_ghost_database will be created
  + resource "azurerm_key_vault_secret" "mysql_ghost_database" {
      + id             = (known after apply)
      + key_vault_id   = (known after apply)
      + name           = "mysql-ghost-database"
      + value          = (sensitive value)
      + version        = (known after apply)
      + versionless_id = (known after apply)
    }

  # azurerm_key_vault_secret.mysql_ghost_password will be created
  + resource "azurerm_key_vault_secret" "mysql_ghost_password" {
      + id             = (known after apply)
      + key_vault_id   = (known after apply)
      + name           = "mysql-ghost-password"
      + value          = (sensitive value)
      + version        = (known after apply)
      + versionless_id = (known after apply)
    }

  # azurerm_key_vault_secret.mysql_ghost_username will be created
  + resource "azurerm_key_vault_secret" "mysql_ghost_username" {
      + id             = (known after apply)
      + key_vault_id   = (known after apply)
      + name           = "mysql-ghost-username"
      + value          = (sensitive value)
      + version        = (known after apply)
      + versionless_id = (known after apply)
    }

  # azurerm_managed_disk.nfs[0] will be created
  + resource "azurerm_managed_disk" "nfs" {
      + create_option        = "Empty"
      + disk_iops_read_write = (known after apply)
      + disk_mbps_read_write = (known after apply)
      + disk_size_gb         = 10
      + id                   = (known after apply)
      + location             = "westeurope"
      + name                 = "vm_k8s_Node01-data"
      + resource_group_name  = "RG_Kubernetes"
      + source_uri           = (known after apply)
      + storage_account_type = "Standard_LRS"
      + tier                 = (known after apply)
    }

  # azurerm_network_interface.nic_k8s_master will be created
  + resource "azurerm_network_interface" "nic_k8s_master" {
      + applied_dns_servers           = (known after apply)
      + dns_servers                   = (known after apply)
      + enable_accelerated_networking = false
      + enable_ip_forwarding          = true
      + id                            = (known after apply)
      + internal_dns_name_label       = (known after apply)
      + internal_domain_name_suffix   = (known after apply)
      + location                      = "westeurope"
      + mac_address                   = (known after apply)
      + name                          = "Master"
      + private_ip_address            = (known after apply)
      + private_ip_addresses          = (known after apply)
      + resource_group_name           = "RG_Kubernetes"
      + tags                          = {
          + "environment" = "K8s"
          + "node"        = "Master"
        }
      + virtual_machine_id            = (known after apply)

      + ip_configuration {
          + name                          = "nic_config_master"
          + primary                       = (known after apply)
          + private_ip_address            = "192.168.1.100"
          + private_ip_address_allocation = "static"
          + private_ip_address_version    = "IPv4"
          + public_ip_address_id          = (known after apply)
          + subnet_id                     = (known after apply)
        }
    }

  # azurerm_network_interface.nic_k8s_node["Node01"] will be created
  + resource "azurerm_network_interface" "nic_k8s_node" {
      + applied_dns_servers           = (known after apply)
      + dns_servers                   = (known after apply)
      + enable_accelerated_networking = false
      + enable_ip_forwarding          = true
      + id                            = (known after apply)
      + internal_dns_name_label       = (known after apply)
      + internal_domain_name_suffix   = (known after apply)
      + location                      = "westeurope"
      + mac_address                   = (known after apply)
      + name                          = "Node01"
      + private_ip_address            = (known after apply)
      + private_ip_addresses          = (known after apply)
      + resource_group_name           = "RG_Kubernetes"
      + tags                          = {
          + "environment" = "K8s"
          + "node"        = "Worker"
        }
      + virtual_machine_id            = (known after apply)

      + ip_configuration {
          + name                          = "nic_config_Node01"
          + primary                       = (known after apply)
          + private_ip_address            = "192.168.1.101"
          + private_ip_address_allocation = "static"
          + private_ip_address_version    = "IPv4"
          + subnet_id                     = (known after apply)
        }
    }

  # azurerm_network_interface.nic_k8s_node["Node02"] will be created
  + resource "azurerm_network_interface" "nic_k8s_node" {
      + applied_dns_servers           = (known after apply)
      + dns_servers                   = (known after apply)
      + enable_accelerated_networking = false
      + enable_ip_forwarding          = true
      + id                            = (known after apply)
      + internal_dns_name_label       = (known after apply)
      + internal_domain_name_suffix   = (known after apply)
      + location                      = "westeurope"
      + mac_address                   = (known after apply)
      + name                          = "Node02"
      + private_ip_address            = (known after apply)
      + private_ip_addresses          = (known after apply)
      + resource_group_name           = "RG_Kubernetes"
      + tags                          = {
          + "environment" = "K8s"
          + "node"        = "Worker"
        }
      + virtual_machine_id            = (known after apply)

      + ip_configuration {
          + name                          = "nic_config_Node02"
          + primary                       = (known after apply)
          + private_ip_address            = "192.168.1.102"
          + private_ip_address_allocation = "static"
          + private_ip_address_version    = "IPv4"
          + subnet_id                     = (known after apply)
        }
    }

  # azurerm_network_interface_security_group_association.nic_nsg_external will be created
  + resource "azurerm_network_interface_security_group_association" "nic_nsg_external" {
      + id                        = (known after apply)
      + network_interface_id      = (known after apply)
      + network_security_group_id = (known after apply)
    }

  # azurerm_network_security_group.nsg_k8s_external will be created
  + resource "azurerm_network_security_group" "nsg_k8s_external" {
      + id                  = (known after apply)
      + location            = "westeurope"
      + name                = "nsg_k8s_external"
      + resource_group_name = "RG_Kubernetes"
      + security_rule       = [
          + {
              + access                                     = "Allow"
              + description                                = "Accept HTTP connections from everyone"
              + destination_address_prefix                 = "*"
              + destination_address_prefixes               = []
              + destination_application_security_group_ids = []
              + destination_port_range                     = "80"
              + destination_port_ranges                    = []
              + direction                                  = "Inbound"
              + name                                       = "HTTP"
              + priority                                   = 1011
              + protocol                                   = "Tcp"
              + source_address_prefix                      = "*"
              + source_address_prefixes                    = []
              + source_application_security_group_ids      = []
              + source_port_range                          = "*"
              + source_port_ranges                         = []
            },
          + {
              + access                                     = "Allow"
              + description                                = "Accept HTTPS connections from everyone"
              + destination_address_prefix                 = "*"
              + destination_address_prefixes               = []
              + destination_application_security_group_ids = []
              + destination_port_range                     = "443"
              + destination_port_ranges                    = []
              + direction                                  = "Inbound"
              + name                                       = "HTTPS"
              + priority                                   = 1021
              + protocol                                   = "Tcp"
              + source_address_prefix                      = "*"
              + source_address_prefixes                    = []
              + source_application_security_group_ids      = []
              + source_port_range                          = "*"
              + source_port_ranges                         = []
            },
          + {
              + access                                     = "Allow"
              + description                                = "Accept SSH connections only from specific address"
              + destination_address_prefix                 = "*"
              + destination_address_prefixes               = []
              + destination_application_security_group_ids = []
              + destination_port_range                     = "22"
              + destination_port_ranges                    = []
              + direction                                  = "Inbound"
              + name                                       = "SSH"
              + priority                                   = 1001
              + protocol                                   = "Tcp"
              + source_address_prefix                      = "149.91.115.233/32"
              + source_address_prefixes                    = []
              + source_application_security_group_ids      = []
              + source_port_range                          = "*"
              + source_port_ranges                         = []
            },
        ]
      + tags                = {
          + "environment" = "K8s"
        }
    }

  # azurerm_private_dns_a_record.master will be created
  + resource "azurerm_private_dns_a_record" "master" {
      + fqdn                = (known after apply)
      + id                  = (known after apply)
      + name                = "master"
      + records             = [
          + "192.168.1.100",
        ]
      + resource_group_name = "RG_Kubernetes"
      + ttl                 = 300
      + zone_name           = "mfvilla.com"
    }

  # azurerm_private_dns_a_record.node["Node01"] will be created
  + resource "azurerm_private_dns_a_record" "node" {
      + fqdn                = (known after apply)
      + id                  = (known after apply)
      + name                = "node01"
      + records             = [
          + "192.168.1.101",
        ]
      + resource_group_name = "RG_Kubernetes"
      + ttl                 = 300
      + zone_name           = "mfvilla.com"
    }

  # azurerm_private_dns_a_record.node["Node02"] will be created
  + resource "azurerm_private_dns_a_record" "node" {
      + fqdn                = (known after apply)
      + id                  = (known after apply)
      + name                = "node02"
      + records             = [
          + "192.168.1.102",
        ]
      + resource_group_name = "RG_Kubernetes"
      + ttl                 = 300
      + zone_name           = "mfvilla.com"
    }

  # azurerm_private_dns_cname_record.nfs[0] will be created
  + resource "azurerm_private_dns_cname_record" "nfs" {
      + fqdn                = (known after apply)
      + id                  = (known after apply)
      + name                = "nfs"
      + record              = (known after apply)
      + resource_group_name = "RG_Kubernetes"
      + ttl                 = 300
      + zone_name           = "mfvilla.com"
    }

  # azurerm_private_dns_zone.k8s_private_dns_zone will be created
  + resource "azurerm_private_dns_zone" "k8s_private_dns_zone" {
      + id                                                    = (known after apply)
      + max_number_of_record_sets                             = (known after apply)
      + max_number_of_virtual_network_links                   = (known after apply)
      + max_number_of_virtual_network_links_with_registration = (known after apply)
      + name                                                  = "mfvilla.com"
      + number_of_record_sets                                 = (known after apply)
      + resource_group_name                                   = "RG_Kubernetes"

      + soa_record {
          + email         = (known after apply)
          + expire_time   = (known after apply)
          + fqdn          = (known after apply)
          + host_name     = (known after apply)
          + minimum_ttl   = (known after apply)
          + refresh_time  = (known after apply)
          + retry_time    = (known after apply)
          + serial_number = (known after apply)
          + tags          = (known after apply)
          + ttl           = (known after apply)
        }
    }

  # azurerm_private_dns_zone_virtual_network_link.vn_dns_link will be created
  + resource "azurerm_private_dns_zone_virtual_network_link" "vn_dns_link" {
      + id                    = (known after apply)
      + name                  = "vn_dns_link"
      + private_dns_zone_name = "mfvilla.com"
      + registration_enabled  = false
      + resource_group_name   = "RG_Kubernetes"
      + virtual_network_id    = (known after apply)
    }

  # azurerm_public_ip.pi_k8s will be created
  + resource "azurerm_public_ip" "pi_k8s" {
      + allocation_method       = "Static"
      + availability_zone       = (known after apply)
      + domain_name_label       = "ghosthope"
      + fqdn                    = (known after apply)
      + id                      = (known after apply)
      + idle_timeout_in_minutes = 4
      + ip_address              = (known after apply)
      + ip_version              = "IPv4"
      + location                = "westeurope"
      + name                    = "pi_k8s"
      + resource_group_name     = "RG_Kubernetes"
      + sku                     = "Standard"
      + tags                    = {
          + "environment" = "K8s"
        }
      + zones                   = (known after apply)
    }

  # azurerm_resource_group.rg_k8s will be created
  + resource "azurerm_resource_group" "rg_k8s" {
      + id       = (known after apply)
      + location = "westeurope"
      + name     = "RG_Kubernetes"
      + tags     = {
          + "environment" = "K8s"
        }
    }

  # azurerm_storage_account.sa_k8s will be created
  + resource "azurerm_storage_account" "sa_k8s" {
      + access_tier                      = (known after apply)
      + account_kind                     = "StorageV2"
      + account_replication_type         = "LRS"
      + account_tier                     = "Standard"
      + allow_blob_public_access         = false
      + enable_https_traffic_only        = true
      + id                               = (known after apply)
      + is_hns_enabled                   = false
      + large_file_share_enabled         = (known after apply)
      + location                         = "westeurope"
      + min_tls_version                  = "TLS1_0"
      + name                             = "mvillasa"
      + nfsv3_enabled                    = false
      + primary_access_key               = (sensitive value)
      + primary_blob_connection_string   = (sensitive value)
      + primary_blob_endpoint            = (known after apply)
      + primary_blob_host                = (known after apply)
      + primary_connection_string        = (sensitive value)
      + primary_dfs_endpoint             = (known after apply)
      + primary_dfs_host                 = (known after apply)
      + primary_file_endpoint            = (known after apply)
      + primary_file_host                = (known after apply)
      + primary_location                 = (known after apply)
      + primary_queue_endpoint           = (known after apply)
      + primary_queue_host               = (known after apply)
      + primary_table_endpoint           = (known after apply)
      + primary_table_host               = (known after apply)
      + primary_web_endpoint             = (known after apply)
      + primary_web_host                 = (known after apply)
      + resource_group_name              = "RG_Kubernetes"
      + secondary_access_key             = (sensitive value)
      + secondary_blob_connection_string = (sensitive value)
      + secondary_blob_endpoint          = (known after apply)
      + secondary_blob_host              = (known after apply)
      + secondary_connection_string      = (sensitive value)
      + secondary_dfs_endpoint           = (known after apply)
      + secondary_dfs_host               = (known after apply)
      + secondary_file_endpoint          = (known after apply)
      + secondary_file_host              = (known after apply)
      + secondary_location               = (known after apply)
      + secondary_queue_endpoint         = (known after apply)
      + secondary_queue_host             = (known after apply)
      + secondary_table_endpoint         = (known after apply)
      + secondary_table_host             = (known after apply)
      + secondary_web_endpoint           = (known after apply)
      + secondary_web_host               = (known after apply)
      + tags                             = {
          + "environment" = "K8s"
        }

      + blob_properties {
          + change_feed_enabled      = (known after apply)
          + default_service_version  = (known after apply)
          + last_access_time_enabled = (known after apply)
          + versioning_enabled       = (known after apply)

          + container_delete_retention_policy {
              + days = (known after apply)
            }

          + cors_rule {
              + allowed_headers    = (known after apply)
              + allowed_methods    = (known after apply)
              + allowed_origins    = (known after apply)
              + exposed_headers    = (known after apply)
              + max_age_in_seconds = (known after apply)
            }

          + delete_retention_policy {
              + days = (known after apply)
            }
        }

      + identity {
          + identity_ids = (known after apply)
          + principal_id = (known after apply)
          + tenant_id    = (known after apply)
          + type         = (known after apply)
        }

      + network_rules {
          + bypass                     = (known after apply)
          + default_action             = (known after apply)
          + ip_rules                   = (known after apply)
          + virtual_network_subnet_ids = (known after apply)

          + private_link_access {
              + endpoint_resource_id = (known after apply)
              + endpoint_tenant_id   = (known after apply)
            }
        }

      + queue_properties {
          + cors_rule {
              + allowed_headers    = (known after apply)
              + allowed_methods    = (known after apply)
              + allowed_origins    = (known after apply)
              + exposed_headers    = (known after apply)
              + max_age_in_seconds = (known after apply)
            }

          + hour_metrics {
              + enabled               = (known after apply)
              + include_apis          = (known after apply)
              + retention_policy_days = (known after apply)
              + version               = (known after apply)
            }

          + logging {
              + delete                = (known after apply)
              + read                  = (known after apply)
              + retention_policy_days = (known after apply)
              + version               = (known after apply)
              + write                 = (known after apply)
            }

          + minute_metrics {
              + enabled               = (known after apply)
              + include_apis          = (known after apply)
              + retention_policy_days = (known after apply)
              + version               = (known after apply)
            }
        }

      + routing {
          + choice                      = (known after apply)
          + publish_internet_endpoints  = (known after apply)
          + publish_microsoft_endpoints = (known after apply)
        }

      + share_properties {
          + cors_rule {
              + allowed_headers    = (known after apply)
              + allowed_methods    = (known after apply)
              + allowed_origins    = (known after apply)
              + exposed_headers    = (known after apply)
              + max_age_in_seconds = (known after apply)
            }

          + retention_policy {
              + days = (known after apply)
            }

          + smb {
              + authentication_types            = (known after apply)
              + channel_encryption_type         = (known after apply)
              + kerberos_ticket_encryption_type = (known after apply)
              + versions                        = (known after apply)
            }
        }
    }

  # azurerm_subnet.sn_k8s_private will be created
  + resource "azurerm_subnet" "sn_k8s_private" {
      + address_prefix                                 = (known after apply)
      + address_prefixes                               = [
          + "192.168.1.0/24",
        ]
      + enforce_private_link_endpoint_network_policies = false
      + enforce_private_link_service_network_policies  = false
      + id                                             = (known after apply)
      + name                                           = "sn_k8s_private"
      + resource_group_name                            = "RG_Kubernetes"
      + virtual_network_name                           = "vn_k8s"
    }

  # azurerm_virtual_machine.vm_k8s_master will be created
  + resource "azurerm_virtual_machine" "vm_k8s_master" {
      + availability_set_id              = (known after apply)
      + delete_data_disks_on_termination = false
      + delete_os_disk_on_termination    = true
      + id                               = (known after apply)
      + license_type                     = (known after apply)
      + location                         = "westeurope"
      + name                             = "vm_k8s_master"
      + network_interface_ids            = (known after apply)
      + primary_network_interface_id     = (known after apply)
      + resource_group_name              = "RG_Kubernetes"
      + tags                             = {
          + "environment" = "K8s"
          + "node"        = "Master"
        }
      + vm_size                          = "Standard_A2_v2"

      + boot_diagnostics {
          + enabled     = true
          + storage_uri = (known after apply)
        }

      + identity {
          + principal_id = (known after apply)
          + type         = "SystemAssigned"
        }

      + os_profile {
          + admin_username = "azureuser"
          + computer_name  = "Master"
          + custom_data    = (known after apply)
        }

      + os_profile_linux_config {
          + disable_password_authentication = true

          + ssh_keys {
              + key_data = (known after apply)
              + path     = "/home/azureuser/.ssh/authorized_keys"
            }
        }

      + storage_data_disk {
          + caching                   = (known after apply)
          + create_option             = (known after apply)
          + disk_size_gb              = (known after apply)
          + lun                       = (known after apply)
          + managed_disk_id           = (known after apply)
          + managed_disk_type         = (known after apply)
          + name                      = (known after apply)
          + vhd_uri                   = (known after apply)
          + write_accelerator_enabled = (known after apply)
        }

      + storage_image_reference {
          + offer     = "debian-10"
          + publisher = "Debian"
          + sku       = "10"
          + version   = "latest"
        }

      + storage_os_disk {
          + caching                   = "ReadWrite"
          + create_option             = "FromImage"
          + disk_size_gb              = (known after apply)
          + managed_disk_id           = (known after apply)
          + managed_disk_type         = "Standard_LRS"
          + name                      = "K8s_Master_OS"
          + os_type                   = (known after apply)
          + write_accelerator_enabled = false
        }
    }

  # azurerm_virtual_machine.vm_k8s_node["Node01"] will be created
  + resource "azurerm_virtual_machine" "vm_k8s_node" {
      + availability_set_id              = (known after apply)
      + delete_data_disks_on_termination = false
      + delete_os_disk_on_termination    = true
      + id                               = (known after apply)
      + license_type                     = (known after apply)
      + location                         = "westeurope"
      + name                             = "vm_k8s_Node01"
      + network_interface_ids            = (known after apply)
      + primary_network_interface_id     = (known after apply)
      + resource_group_name              = "RG_Kubernetes"
      + tags                             = {
          + "environment" = "K8s"
          + "node"        = "Worker"
        }
      + vm_size                          = "Standard_A1_v2"

      + boot_diagnostics {
          + enabled     = true
          + storage_uri = (known after apply)
        }

      + identity {
          + identity_ids = (known after apply)
          + principal_id = (known after apply)
          + type         = (known after apply)
        }

      + os_profile {
          + admin_username = "azureuser"
          + computer_name  = "Node01"
          + custom_data    = (known after apply)
        }

      + os_profile_linux_config {
          + disable_password_authentication = true

          + ssh_keys {
              + key_data = (known after apply)
              + path     = "/home/azureuser/.ssh/authorized_keys"
            }
        }

      + storage_data_disk {
          + caching                   = (known after apply)
          + create_option             = (known after apply)
          + disk_size_gb              = (known after apply)
          + lun                       = (known after apply)
          + managed_disk_id           = (known after apply)
          + managed_disk_type         = (known after apply)
          + name                      = (known after apply)
          + vhd_uri                   = (known after apply)
          + write_accelerator_enabled = (known after apply)
        }

      + storage_image_reference {
          + offer     = "debian-10"
          + publisher = "Debian"
          + sku       = "10"
          + version   = "latest"
        }

      + storage_os_disk {
          + caching                   = "ReadWrite"
          + create_option             = "FromImage"
          + disk_size_gb              = (known after apply)
          + managed_disk_id           = (known after apply)
          + managed_disk_type         = "Standard_LRS"
          + name                      = "K8s_Node01_OS"
          + os_type                   = (known after apply)
          + write_accelerator_enabled = false
        }
    }

  # azurerm_virtual_machine.vm_k8s_node["Node02"] will be created
  + resource "azurerm_virtual_machine" "vm_k8s_node" {
      + availability_set_id              = (known after apply)
      + delete_data_disks_on_termination = false
      + delete_os_disk_on_termination    = true
      + id                               = (known after apply)
      + license_type                     = (known after apply)
      + location                         = "westeurope"
      + name                             = "vm_k8s_Node02"
      + network_interface_ids            = (known after apply)
      + primary_network_interface_id     = (known after apply)
      + resource_group_name              = "RG_Kubernetes"
      + tags                             = {
          + "environment" = "K8s"
          + "node"        = "Worker"
        }
      + vm_size                          = "Standard_A1_v2"

      + boot_diagnostics {
          + enabled     = true
          + storage_uri = (known after apply)
        }

      + identity {
          + identity_ids = (known after apply)
          + principal_id = (known after apply)
          + type         = (known after apply)
        }

      + os_profile {
          + admin_username = "azureuser"
          + computer_name  = "Node02"
          + custom_data    = (known after apply)
        }

      + os_profile_linux_config {
          + disable_password_authentication = true

          + ssh_keys {
              + key_data = (known after apply)
              + path     = "/home/azureuser/.ssh/authorized_keys"
            }
        }

      + storage_data_disk {
          + caching                   = (known after apply)
          + create_option             = (known after apply)
          + disk_size_gb              = (known after apply)
          + lun                       = (known after apply)
          + managed_disk_id           = (known after apply)
          + managed_disk_type         = (known after apply)
          + name                      = (known after apply)
          + vhd_uri                   = (known after apply)
          + write_accelerator_enabled = (known after apply)
        }

      + storage_image_reference {
          + offer     = "debian-10"
          + publisher = "Debian"
          + sku       = "10"
          + version   = "latest"
        }

      + storage_os_disk {
          + caching                   = "ReadWrite"
          + create_option             = "FromImage"
          + disk_size_gb              = (known after apply)
          + managed_disk_id           = (known after apply)
          + managed_disk_type         = "Standard_LRS"
          + name                      = "K8s_Node02_OS"
          + os_type                   = (known after apply)
          + write_accelerator_enabled = false
        }
    }

  # azurerm_virtual_machine_data_disk_attachment.nfs[0] will be created
  + resource "azurerm_virtual_machine_data_disk_attachment" "nfs" {
      + caching                   = "ReadWrite"
      + create_option             = "Attach"
      + id                        = (known after apply)
      + lun                       = 10
      + managed_disk_id           = (known after apply)
      + virtual_machine_id        = (known after apply)
      + write_accelerator_enabled = false
    }

  # azurerm_virtual_network.vn_k8s will be created
  + resource "azurerm_virtual_network" "vn_k8s" {
      + address_space         = [
          + "192.168.0.0/16",
        ]
      + guid                  = (known after apply)
      + id                    = (known after apply)
      + location              = "westeurope"
      + name                  = "vn_k8s"
      + resource_group_name   = "RG_Kubernetes"
      + subnet                = (known after apply)
      + tags                  = {
          + "environment" = "K8s"
        }
      + vm_protection_enabled = false
    }

  # local_file.external_pem[0] will be created
  + resource "local_file" "external_pem" {
      + content              = (sensitive)
      + directory_permission = "0700"
      + file_permission      = "0400"
      + filename             = "./resources/external.pem"
      + id                   = (known after apply)
    }

  # local_file.internal_pem[0] will be created
  + resource "local_file" "internal_pem" {
      + content              = (sensitive)
      + directory_permission = "0700"
      + file_permission      = "0400"
      + filename             = "./resources/internal.pem"
      + id                   = (known after apply)
    }

  # null_resource.ansible will be created
  + resource "null_resource" "ansible" {
      + id = (known after apply)
    }

  # random_id.randomId will be created
  + resource "random_id" "randomId" {
      + b64_std     = (known after apply)
      + b64_url     = (known after apply)
      + byte_length = 8
      + dec         = (known after apply)
      + hex         = (known after apply)
      + id          = (known after apply)
      + keepers     = {
          + "resource_group" = "RG_Kubernetes"
        }
    }

  # random_string.mysql_admin_password will be created
  + resource "random_string" "mysql_admin_password" {
      + id               = (known after apply)
      + length           = 18
      + lower            = true
      + min_lower        = 2
      + min_numeric      = 2
      + min_special      = 2
      + min_upper        = 2
      + number           = true
      + override_special = "/@-.,_"
      + result           = (known after apply)
      + special          = true
      + upper            = true
    }

  # random_string.mysql_ghost_password will be created
  + resource "random_string" "mysql_ghost_password" {
      + id               = (known after apply)
      + length           = 18
      + lower            = true
      + min_lower        = 2
      + min_numeric      = 2
      + min_special      = 2
      + min_upper        = 2
      + number           = true
      + override_special = "/@-.,_"
      + result           = (known after apply)
      + special          = true
      + upper            = true
    }

  # random_string.mysql_ghost_username will be created
  + resource "random_string" "mysql_ghost_username" {
      + id          = (known after apply)
      + length      = 8
      + lower       = true
      + min_lower   = 2
      + min_numeric = 2
      + min_special = 0
      + min_upper   = 2
      + number      = true
      + result      = (known after apply)
      + special     = false
      + upper       = true
    }

  # tls_private_key.external_ssh will be created
  + resource "tls_private_key" "external_ssh" {
      + algorithm                  = "RSA"
      + ecdsa_curve                = "P224"
      + id                         = (known after apply)
      + private_key_pem            = (sensitive value)
      + public_key_fingerprint_md5 = (known after apply)
      + public_key_openssh         = (known after apply)
      + public_key_pem             = (known after apply)
      + rsa_bits                   = 4096
    }

  # tls_private_key.internal_ssh will be created
  + resource "tls_private_key" "internal_ssh" {
      + algorithm                  = "RSA"
      + ecdsa_curve                = "P224"
      + id                         = (known after apply)
      + private_key_pem            = (sensitive value)
      + public_key_fingerprint_md5 = (known after apply)
      + public_key_openssh         = (known after apply)
      + public_key_pem             = (known after apply)
      + rsa_bits                   = 4096
    }

Plan: 37 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + ansible_exec_command    = (known after apply)
  + ghost_http_service_url  = (known after apply)
  + ghost_https_service_url = (known after apply)
  + master_private_address  = (known after apply)
  + master_public_address   = (known after apply)
  + node01_private_address  = (known after apply)
  + node02_private_address  = (known after apply)
  + ssh_admin_user          = "azureuser"
  + ssh_master_connection   = (known after apply)
  + ssh_node01_connection   = (known after apply)
  + ssh_node02_connection   = (known after apply)
  + subnet_cidr_private     = [
      + "192.168.1.0/24",
    ]
  + virtual_network_cidr    = [
      + "192.168.0.0/16",
    ]
random_string.mysql_ghost_username: Creating...
random_string.mysql_admin_password: Creating...
random_string.mysql_ghost_password: Creating...
random_string.mysql_ghost_username: Creation complete after 0s [id=40FAhh8s]
random_string.mysql_admin_password: Creation complete after 0s [id=b-ZzxZ7c7x,FBFA9xS]
random_string.mysql_ghost_password: Creation complete after 0s [id=7vL4@tMjEqf7oZ-Pqz]
tls_private_key.internal_ssh: Creating...
tls_private_key.external_ssh: Creating...
tls_private_key.external_ssh: Creation complete after 2s [id=c7ee6a8b17750660099a84405f957cd347efe686]
local_file.external_pem[0]: Creating...
local_file.external_pem[0]: Creation complete after 0s [id=7a6bde4753c64e4bfed4bbc268dbce1b2e8b8732]
tls_private_key.internal_ssh: Creation complete after 2s [id=d9dbd15172ed737fdc28bbfe3ac234da808766e8]
local_file.internal_pem[0]: Creating...
local_file.internal_pem[0]: Creation complete after 0s [id=9fdf2b21acf53ff122900dc140d551512083407f]
azurerm_resource_group.rg_k8s: Creating...
azurerm_resource_group.rg_k8s: Creation complete after 1s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes]
azurerm_virtual_network.vn_k8s: Creating...
random_id.randomId: Creating...
azurerm_key_vault.keyvault_k8s: Creating...
azurerm_public_ip.pi_k8s: Creating...
azurerm_private_dns_zone.k8s_private_dns_zone: Creating...
random_id.randomId: Creation complete after 0s [id=30174M09MuE]
azurerm_network_security_group.nsg_k8s_external: Creating...
azurerm_storage_account.sa_k8s: Creating...
azurerm_virtual_network.vn_k8s: Creation complete after 5s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/virtualNetworks/vn_k8s]
azurerm_subnet.sn_k8s_private: Creating...
azurerm_network_security_group.nsg_k8s_external: Creation complete after 5s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/networkSecurityGroups/nsg_k8s_external]
azurerm_public_ip.pi_k8s: Creation complete after 5s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/publicIPAddresses/pi_k8s]
azurerm_subnet.sn_k8s_private: Creation complete after 4s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/virtualNetworks/vn_k8s/subnets/sn_k8s_private]
azurerm_network_interface.nic_k8s_node["Node02"]: Creating...
azurerm_network_interface.nic_k8s_master: Creating...
azurerm_network_interface.nic_k8s_node["Node01"]: Creating...
azurerm_key_vault.keyvault_k8s: Still creating... [10s elapsed]
azurerm_private_dns_zone.k8s_private_dns_zone: Still creating... [10s elapsed]
azurerm_storage_account.sa_k8s: Still creating... [10s elapsed]
azurerm_network_interface.nic_k8s_node["Node02"]: Creation complete after 1s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/networkInterfaces/Node02]
azurerm_network_interface.nic_k8s_node["Node01"]: Creation complete after 2s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/networkInterfaces/Node01]
azurerm_network_interface.nic_k8s_master: Creation complete after 3s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/networkInterfaces/Master]
azurerm_network_interface_security_group_association.nic_nsg_external: Creating...
azurerm_network_interface_security_group_association.nic_nsg_external: Creation complete after 1s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/networkInterfaces/Master|/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/networkSecurityGroups/nsg_k8s_external]
azurerm_key_vault.keyvault_k8s: Still creating... [20s elapsed]
azurerm_private_dns_zone.k8s_private_dns_zone: Still creating... [20s elapsed]
azurerm_storage_account.sa_k8s: Still creating... [20s elapsed]
azurerm_storage_account.sa_k8s: Creation complete after 27s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Storage/storageAccounts/mvillasa]
azurerm_virtual_machine.vm_k8s_master: Creating...
azurerm_virtual_machine.vm_k8s_node["Node01"]: Creating...
azurerm_virtual_machine.vm_k8s_node["Node02"]: Creating...
azurerm_key_vault.keyvault_k8s: Still creating... [30s elapsed]
azurerm_private_dns_zone.k8s_private_dns_zone: Still creating... [30s elapsed]
azurerm_private_dns_zone.k8s_private_dns_zone: Creation complete after 33s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/privateDnsZones/mfvilla.com]
azurerm_private_dns_a_record.node["Node02"]: Creating...
azurerm_private_dns_zone_virtual_network_link.vn_dns_link: Creating...
azurerm_private_dns_a_record.node["Node01"]: Creating...
azurerm_private_dns_a_record.master: Creating...
azurerm_private_dns_a_record.node["Node01"]: Creation complete after 0s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/privateDnsZones/mfvilla.com/A/node01]
azurerm_private_dns_cname_record.nfs[0]: Creating...
azurerm_private_dns_a_record.node["Node02"]: Creation complete after 1s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/privateDnsZones/mfvilla.com/A/node02]
azurerm_private_dns_a_record.master: Creation complete after 1s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/privateDnsZones/mfvilla.com/A/master]
azurerm_private_dns_cname_record.nfs[0]: Creation complete after 2s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/privateDnsZones/mfvilla.com/CNAME/nfs]
azurerm_virtual_machine.vm_k8s_master: Still creating... [10s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [10s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node02"]: Still creating... [10s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [40s elapsed]
azurerm_private_dns_zone_virtual_network_link.vn_dns_link: Still creating... [10s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [20s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [20s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node02"]: Still creating... [20s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [50s elapsed]
azurerm_private_dns_zone_virtual_network_link.vn_dns_link: Still creating... [20s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [30s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [30s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node02"]: Still creating... [30s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [1m0s elapsed]
azurerm_private_dns_zone_virtual_network_link.vn_dns_link: Still creating... [30s elapsed]
azurerm_private_dns_zone_virtual_network_link.vn_dns_link: Creation complete after 33s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/privateDnsZones/mfvilla.com/virtualNetworkLinks/vn_dns_link]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [40s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [40s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node02"]: Still creating... [40s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [1m10s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node02"]: Creation complete after 48s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Compute/virtualMachines/vm_k8s_Node02]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [50s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [50s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [1m20s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [1m0s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [1m0s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [1m30s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [1m10s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [1m10s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [1m40s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [1m20s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [1m20s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [1m50s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [1m30s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [1m30s elapsed]
azurerm_key_vault.keyvault_k8s: Still creating... [2m0s elapsed]
azurerm_virtual_machine.vm_k8s_master: Still creating... [1m40s elapsed]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Still creating... [1m40s elapsed]
azurerm_key_vault.keyvault_k8s: Creation complete after 2m8s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.KeyVault/vaults/keyvaultk8s]
azurerm_key_vault_access_policy.keyvault_k8s_rw: Creating...
azurerm_key_vault_access_policy.keyvault_k8s_rw: Creation complete after 6s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.KeyVault/vaults/keyvaultk8s/objectId/af6b03e0-8970-4010-a204-ed0b33702065]
azurerm_virtual_machine.vm_k8s_node["Node01"]: Creation complete after 1m48s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Compute/virtualMachines/vm_k8s_Node01]
azurerm_managed_disk.nfs[0]: Creating...
azurerm_virtual_machine.vm_k8s_master: Creation complete after 1m49s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Compute/virtualMachines/vm_k8s_master]
data.azurerm_public_ip.pi_k8s: Reading...
azurerm_key_vault_access_policy.keyvault_k8s_ro: Creating...
data.azurerm_public_ip.pi_k8s: Read complete after 0s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Network/publicIPAddresses/pi_k8s]
azurerm_managed_disk.nfs[0]: Creation complete after 3s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Compute/disks/vm_k8s_Node01-data]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Creating...
azurerm_key_vault_access_policy.keyvault_k8s_ro: Creation complete after 7s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.KeyVault/vaults/keyvaultk8s/objectId/05c98608-4638-4104-bd5e-a04f0593eb93]
azurerm_key_vault_secret.mysql_ghost_username: Creating...
azurerm_key_vault_secret.mysql_ghost_database: Creating...
azurerm_key_vault_secret.mysql_admin_password: Creating...
azurerm_key_vault_secret.mysql_ghost_password: Creating...
azurerm_key_vault_secret.mysql_ghost_username: Creation complete after 1s [id=https://keyvaultk8s.vault.azure.net/secrets/mysql-ghost-username/69fe10e288a243d381dbd028bb910d88]
azurerm_key_vault_secret.mysql_ghost_database: Creation complete after 1s [id=https://keyvaultk8s.vault.azure.net/secrets/mysql-ghost-database/98cba0912c6f430d87c3733c75dd0fab]
azurerm_key_vault_secret.mysql_admin_password: Creation complete after 2s [id=https://keyvaultk8s.vault.azure.net/secrets/mysql-admin-password/b7545ea635784e84a58b5d380d151aa8]
azurerm_key_vault_secret.mysql_ghost_password: Creation complete after 2s [id=https://keyvaultk8s.vault.azure.net/secrets/mysql-ghost-password/ef418e72d3e846c4863a8dff60761402]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Still creating... [10s elapsed]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Still creating... [20s elapsed]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Still creating... [30s elapsed]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Still creating... [40s elapsed]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Still creating... [50s elapsed]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Still creating... [1m0s elapsed]
azurerm_virtual_machine_data_disk_attachment.nfs[0]: Creation complete after 1m2s [id=/subscriptions/fa4d5daf-6219-4ac5-8718-a1ff86d3fca7/resourceGroups/RG_Kubernetes/providers/Microsoft.Compute/virtualMachines/vm_k8s_Node01/dataDisks/vm_k8s_Node01-data]
null_resource.ansible: Creating...
null_resource.ansible: Provisioning with 'local-exec'...
null_resource.ansible (local-exec): Executing: ["/bin/bash" "-c" "ansible-playbook -e jump_host='20.86.240.158' -e admin_user='azureuser' -e subnet_cidr_private='192.168.1.0/24' -e private_lan_master='192.168.1.100' -e private_lan_node01='192.168.1.101' -e private_lan_node02='192.168.1.102' -e internal_private_key_file='resources/internal.pem' -e external_private_key_file='resources/external.pem' -e ghost_url='ghosthope.westeurope.cloudapp.azure.com' -e prefix='mfvilla' ../ansible/playbooks/create.yml\n"]
null_resource.ansible (local-exec): [DEPRECATION WARNING]: Ansible will require Python 3.8 or newer on the
null_resource.ansible (local-exec): controller starting with Ansible 2.12. Current version: 3.7.3 (default, Jan 22
null_resource.ansible (local-exec): 2021, 20:04:44) [GCC 8.3.0]. This feature will be removed from ansible-core in
null_resource.ansible (local-exec): version 2.12. Deprecation warnings can be disabled by setting
null_resource.ansible (local-exec): deprecation_warnings=False in ansible.cfg.

null_resource.ansible (local-exec): PLAY [all] *********************************************************************

null_resource.ansible (local-exec): TASK [common : include_tasks] **************************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/common/tasks/create.yml for Master, Node01
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/common/tasks/create.yml for Node02
The authenticity of host '20.86.240.158 (20.86.240.158)' can't be established.
ECDSA key fingerprint is SHA256:TrGID878vUCGU8I7ZVPZ01JVyDG/GvQvKOJiCVmOrTU.
Are you sure you want to continue connecting (yes/no)? null_resource.ansible: Still creating... [10s elapsed]

null_resource.ansible (local-exec): TASK [common : Correct python version to always execute python3] ***************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [common : Correct python version to always execute python3] ***************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible: Still creating... [20s elapsed]

null_resource.ansible (local-exec): TASK [common : Update apt repository cache] ************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [common : Update apt repository cache] ************************************
null_resource.ansible (local-exec): ok: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible: Still creating... [30s elapsed]

null_resource.ansible (local-exec): TASK [common : Correct python version to always execute python3] ***************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Update apt repository cache] ************************************
null_resource.ansible (local-exec): ok: [Node01]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible: Still creating... [50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [1m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible: Still creating... [1m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible: Still creating... [1m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [1m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [1m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [1m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [2m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [2m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [2m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible: Still creating... [2m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [2m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=839446021335.1586 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Install Common packages] ****************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [2m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible: Still creating... [3m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [3m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible: Still creating... [3m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=336497985193.1539 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Install Common packages] ****************************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible: Still creating... [3m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=441201498383.1546 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible: Still creating... [3m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Install Common packages] ****************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible: Still creating... [3m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible: Still creating... [4m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [4m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible: Still creating... [4m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [4m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [4m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible: Still creating... [4m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=928836983374.9341 started=1 finished=0
null_resource.ansible: Still creating... [5m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Install pip packages] *******************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [5m10s elapsed]

null_resource.ansible (local-exec): TASK [common : Set vim as default editor] **************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [5m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Set timezone to Europe/Madrid] **********************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Enable Chrony service] ******************************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible: Still creating... [5m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Ensure Chrony service is running] *******************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [common : include_tasks] **************************************************
null_resource.ansible (local-exec): skipping: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible: Still creating... [5m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible: Still creating... [5m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=72106830072.9324 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [6m0s elapsed]

null_resource.ansible (local-exec): TASK [common : Install pip packages] *******************************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [6m10s elapsed]

null_resource.ansible (local-exec): TASK [common : Set vim as default editor] **************************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0
null_resource.ansible: Still creating... [6m20s elapsed]

null_resource.ansible (local-exec): TASK [common : Set timezone to Europe/Madrid] **********************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=953116370347.9331 started=1 finished=0

null_resource.ansible (local-exec): TASK [common : Install pip packages] *******************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [6m30s elapsed]

null_resource.ansible (local-exec): TASK [common : Enable Chrony service] ******************************************
null_resource.ansible (local-exec): ok: [Node02]

null_resource.ansible (local-exec): TASK [common : Set vim as default editor] **************************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [common : Ensure Chrony service is running] *******************************
null_resource.ansible (local-exec): ok: [Node02]

null_resource.ansible (local-exec): TASK [common : include_tasks] **************************************************
null_resource.ansible (local-exec): skipping: [Node02]
null_resource.ansible: Still creating... [6m40s elapsed]

null_resource.ansible (local-exec): TASK [common : Set timezone to Europe/Madrid] **********************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [6m50s elapsed]

null_resource.ansible (local-exec): TASK [common : Enable Chrony service] ******************************************
null_resource.ansible (local-exec): ok: [Node01]
null_resource.ansible: Still creating... [7m0s elapsed]

null_resource.ansible (local-exec): TASK [common : Ensure Chrony service is running] *******************************
null_resource.ansible (local-exec): ok: [Node01]

null_resource.ansible (local-exec): TASK [common : include_tasks] **************************************************
null_resource.ansible (local-exec): skipping: [Node01]

null_resource.ansible (local-exec): PLAY [nfs] *********************************************************************

null_resource.ansible (local-exec): TASK [nfs : include_tasks] *****************************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/nfs/tasks/create.yml for Node01

null_resource.ansible (local-exec): TASK [nfs : Make sure mount point exists] **************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [7m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=151449120919.9905 started=1 finished=0
null_resource.ansible: Still creating... [7m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=151449120919.9905 started=1 finished=0
null_resource.ansible: Still creating... [7m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=151449120919.9905 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=151449120919.9905 started=1 finished=0
null_resource.ansible: Still creating... [7m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=151449120919.9905 started=1 finished=0
null_resource.ansible: Still creating... [7m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=151449120919.9905 started=1 finished=0
null_resource.ansible: Still creating... [8m0s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Install LVM and NFS packages] **************************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [nfs : Make NFS to run in private LAN only] *******************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [8m10s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Read device information] *******************************************
null_resource.ansible (local-exec): ok: [Node01]

null_resource.ansible (local-exec): TASK [nfs : Debug sdc_info] ****************************************************
null_resource.ansible (local-exec): ok: [Node01] => {
null_resource.ansible (local-exec):     "sdc_info.partitions": []
null_resource.ansible (local-exec): }
null_resource.ansible: Still creating... [8m20s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Create a new primary partition] ************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [8m30s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Create Volume Group on /dev/sdc with physical extent size = 10g] ***
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [8m40s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Create a logical volume] *******************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [8m50s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Create xfs filesystem on nfs data partition] ***********************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [9m0s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Check if /dev/nfs-vg/nfs-lv is in fstab] ***************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [9m10s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Make sure /srv/nfs is mounted] *************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [9m20s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Make sure exportfs accepts LAN] ************************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [nfs : Restart nfs-kernel-server.service] *********************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [9m30s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Re-export the share] ***********************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [9m40s elapsed]

null_resource.ansible (local-exec): TASK [nfs : Make sure fs structure for ghost data exists] **********************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [nfs : include_tasks] *****************************************************
null_resource.ansible (local-exec): skipping: [Node01]

null_resource.ansible (local-exec): PLAY [deployer] ****************************************************************

null_resource.ansible (local-exec): TASK [deployment-tools : include_tasks] ****************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/deployment-tools/tasks/create.yml for Master
null_resource.ansible: Still creating... [9m50s elapsed]

null_resource.ansible (local-exec): TASK [deployment-tools : Create .kube diretory] ********************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [10m0s elapsed]

null_resource.ansible (local-exec): TASK [deployment-tools : Uncompress helm] **************************************
null_resource.ansible (local-exec): [DEPRECATION WARNING]: Distribution debian 10.10 on host Master should use
null_resource.ansible (local-exec): /usr/bin/python3, but is using /usr/bin/python for backward compatibility with
null_resource.ansible (local-exec): prior Ansible releases. A future Ansible release will default to using the
null_resource.ansible (local-exec): discovered platform python for this host. See https://docs.ansible.com/ansible/
null_resource.ansible (local-exec): 2.11/reference_appendices/interpreter_discovery.html for more information. This
null_resource.ansible (local-exec):  feature will be removed in version 2.12. Deprecation warnings can be disabled
null_resource.ansible (local-exec): by setting deprecation_warnings=False in ansible.cfg.
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [deployment-tools : Copy helm to /usr/local/bin/helm] *********************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [deployment-tools : Remove unarchived helm directory] *********************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [10m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible: Still creating... [10m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible: Still creating... [10m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible: Still creating... [10m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible: Still creating... [10m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0
null_resource.ansible: Still creating... [11m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=68571978315.10044 started=1 finished=0

null_resource.ansible (local-exec): TASK [deployment-tools : Install pip files] ************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [11m10s elapsed]

null_resource.ansible (local-exec): TASK [deployment-tools : Create directory for secure storing get_keyvault_value script] ***
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [deployment-tools : Copy password obtainer] *******************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [deployment-tools : include_tasks] ****************************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): PLAY [kubernetes] **************************************************************

null_resource.ansible (local-exec): TASK [docker : include_tasks] **************************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/docker/tasks/create.yml for Master, Node01
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/docker/tasks/create.yml for Node02
null_resource.ansible: Still creating... [11m20s elapsed]

null_resource.ansible (local-exec): TASK [docker : Gather the package facts] ***************************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible (local-exec): ok: [Node01]

null_resource.ansible (local-exec): TASK [docker : Gather the package facts] ***************************************
null_resource.ansible (local-exec): ok: [Node02]
null_resource.ansible: Still creating... [11m30s elapsed]

null_resource.ansible (local-exec): TASK [docker : Download Docker installation script] ****************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [docker : Download Docker installation script] ****************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible: Still creating... [11m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible: Still creating... [11m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible: Still creating... [12m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible: Still creating... [12m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible: Still creating... [12m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible: Still creating... [12m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible: Still creating... [12m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible: Still creating... [12m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=532424890143.10392 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0

null_resource.ansible (local-exec): TASK [docker : Install Docker] *************************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible: Still creating... [13m0s elapsed]

null_resource.ansible (local-exec): TASK [docker : Enable Docker service] ******************************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible: Still creating... [13m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0

null_resource.ansible (local-exec): TASK [docker : Ensure Docker service is running] *******************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=951605518893.14408 started=1 finished=0
null_resource.ansible: Still creating... [13m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0

null_resource.ansible (local-exec): TASK [docker : Remove Docker installation script] ******************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=156927256732.9990 started=1 finished=0

null_resource.ansible (local-exec): TASK [docker : Add azureuser to docker group] **********************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [docker : Install Docker] *************************************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [docker : include_tasks] **************************************************
null_resource.ansible (local-exec): skipping: [Master]
null_resource.ansible: Still creating... [13m30s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : include_tasks] **********************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/kubernetes/tasks/create.yml for Master

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/modprobe.d/k8s.conf] ******************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [docker : Install Docker] *************************************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [docker : Enable Docker service] ******************************************
null_resource.ansible (local-exec): ok: [Node01]
null_resource.ansible: Still creating... [13m40s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Add the br_netfilter module] ********************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [docker : Enable Docker service] ******************************************
null_resource.ansible (local-exec): ok: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/sysctl.d/k8s.conf] ********************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [docker : Ensure Docker service is running] *******************************
null_resource.ansible (local-exec): ok: [Node01]
null_resource.ansible: Still creating... [13m50s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Make sure syctl entries are loaded] *************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [docker : Ensure Docker service is running] *******************************
null_resource.ansible (local-exec): ok: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : Make sure syctl entries are loaded] *************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [docker : Remove Docker installation script] ******************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [14m0s elapsed]

null_resource.ansible (local-exec): TASK [docker : Remove Docker installation script] ******************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : Add apt key] ************************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [docker : Add azureuser to docker group] **********************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [docker : include_tasks] **************************************************
null_resource.ansible (local-exec): skipping: [Node01]

null_resource.ansible (local-exec): TASK [kubernetes : include_tasks] **********************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/kubernetes/tasks/create.yml for Node01

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/apt/sources.list.d/kubernetes.list] ***************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [14m10s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/modprobe.d/k8s.conf] ******************************
null_resource.ansible (local-exec): [DEPRECATION WARNING]: Distribution debian 10.10 on host Node01 should use
null_resource.ansible (local-exec): /usr/bin/python3, but is using /usr/bin/python for backward compatibility with
null_resource.ansible (local-exec): prior Ansible releases. A future Ansible release will default to using the
null_resource.ansible (local-exec): discovered platform python for this host. See https://docs.ansible.com/ansible/
null_resource.ansible (local-exec): 2.11/reference_appendices/interpreter_discovery.html for more information. This
null_resource.ansible (local-exec):  feature will be removed in version 2.12. Deprecation warnings can be disabled
null_resource.ansible (local-exec): by setting deprecation_warnings=False in ansible.cfg.
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [docker : Add azureuser to docker group] **********************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [docker : include_tasks] **************************************************
null_resource.ansible (local-exec): skipping: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : include_tasks] **********************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/kubernetes/tasks/create.yml for Node02
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=268860143792.13571 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/modprobe.d/k8s.conf] ******************************
null_resource.ansible (local-exec): [DEPRECATION WARNING]: Distribution debian 10.10 on host Node02 should use
null_resource.ansible (local-exec): /usr/bin/python3, but is using /usr/bin/python for backward compatibility with
null_resource.ansible (local-exec): prior Ansible releases. A future Ansible release will default to using the
null_resource.ansible (local-exec): discovered platform python for this host. See https://docs.ansible.com/ansible/
null_resource.ansible (local-exec): 2.11/reference_appendices/interpreter_discovery.html for more information. This
null_resource.ansible (local-exec):  feature will be removed in version 2.12. Deprecation warnings can be disabled
null_resource.ansible (local-exec): by setting deprecation_warnings=False in ansible.cfg.
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=268860143792.13571 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Add the br_netfilter module] ********************************
null_resource.ansible (local-exec): ok: [Node01]
null_resource.ansible: Still creating... [14m20s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Update apt repositories cache] ******************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/sysctl.d/k8s.conf] ********************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [kubernetes : Add the br_netfilter module] ********************************
null_resource.ansible (local-exec): ok: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/sysctl.d/k8s.conf] ********************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=36919860402.14122 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Make sure syctl entries are loaded] *************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [14m30s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Make sure syctl entries are loaded] *************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=36919860402.14122 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Make sure syctl entries are loaded] *************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [14m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=36919860402.14122 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Make sure syctl entries are loaded] *************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : Add apt key] ************************************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/apt/sources.list.d/kubernetes.list] ***************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [kubernetes : Add apt key] ************************************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible: Still creating... [14m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=36919860402.14122 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Copy /etc/apt/sources.list.d/kubernetes.list] ***************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=529233332116.17594 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=36919860402.14122 started=1 finished=0
null_resource.ansible: Still creating... [15m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=466497813612.13206 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=529233332116.17594 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=36919860402.14122 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=466497813612.13206 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Update apt repositories cache] ******************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible: Still creating... [15m10s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Install Kubernetes] *****************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubernetes : Update apt repositories cache] ******************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Master] => (item=kubelet)
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0
null_resource.ansible: Still creating... [15m20s elapsed]
null_resource.ansible (local-exec): changed: [Master] => (item=kubeadm)
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible: Still creating... [15m30s elapsed]
null_resource.ansible (local-exec): changed: [Master] => (item=kubectl)
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Set bash completion for kubernetes] *************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubernetes : include_tasks] **********************************************
null_resource.ansible (local-exec): skipping: [Master]
null_resource.ansible: Still creating... [15m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible: Still creating... [15m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0
null_resource.ansible: Still creating... [16m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=21450355119.18139 started=1 finished=0
null_resource.ansible: Still creating... [16m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubernetes : Install Kubernetes] *****************************************
null_resource.ansible (local-exec): changed: [Node01]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=879760613312.13757 started=1 finished=0
null_resource.ansible: Still creating... [16m20s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Node01] => (item=kubelet)

null_resource.ansible (local-exec): TASK [kubernetes : Install Kubernetes] *****************************************
null_resource.ansible (local-exec): changed: [Node02]
null_resource.ansible: Still creating... [16m30s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Node01] => (item=kubeadm)

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Node02] => (item=kubelet)

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Node01] => (item=kubectl)
null_resource.ansible: Still creating... [16m40s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Node02] => (item=kubeadm)

null_resource.ansible (local-exec): TASK [kubernetes : Set bash completion for kubernetes] *************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [kubernetes : include_tasks] **********************************************
null_resource.ansible (local-exec): skipping: [Node01]
null_resource.ansible: Still creating... [16m50s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Hold kubernetes packages] ***********************************
null_resource.ansible (local-exec): changed: [Node02] => (item=kubectl)
null_resource.ansible: Still creating... [17m0s elapsed]

null_resource.ansible (local-exec): TASK [kubernetes : Set bash completion for kubernetes] *************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [kubernetes : include_tasks] **********************************************
null_resource.ansible (local-exec): skipping: [Node02]

null_resource.ansible (local-exec): PLAY [kubemasters] *************************************************************

null_resource.ansible (local-exec): TASK [Gathering Facts] *********************************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : include_tasks] *********************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/kubemasters/tasks/create.yml for Master

null_resource.ansible (local-exec): TASK [kubemasters : Copy rbac.yaml to /tmp/rbac.yaml] **************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Debug kubernetes state before init] ************************
null_resource.ansible (local-exec): fatal: [Master]: FAILED! => {"changed": true, "cmd": "kubectl cluster-info dump", "delta": "0:00:00.129429", "end": "2021-06-24 20:54:36.861801", "msg": "non-zero return code", "rc": 1, "start": "2021-06-24 20:54:36.732372", "stderr": "The connection to the server localhost:8080 was refused - did you specify the right host or port?", "stderr_lines": ["The connection to the server localhost:8080 was refused - did you specify the right host or port?"], "stdout": "", "stdout_lines": []}
null_resource.ansible (local-exec): ...ignoring
null_resource.ansible: Still creating... [17m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [17m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [17m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [17m40s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [17m50s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m0s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m10s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m20s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m30s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m39s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m49s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Master: jid=853542092231.14925 started=1 finished=0
null_resource.ansible: Still creating... [18m59s elapsed]

null_resource.ansible (local-exec): TASK [kubemasters : Init K8s] **************************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Create .kube directory] ************************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible: Still creating... [19m9s elapsed]

null_resource.ansible (local-exec): TASK [kubemasters : Make sure /root/.kube directory exists] ********************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Copy k8s config] *******************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Copy k8s config to local /root/.kube directory] ************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [19m19s elapsed]

null_resource.ansible (local-exec): TASK [kubemasters : Get Join command] ******************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Register Join command] *************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Debug join command] ****************************************
null_resource.ansible (local-exec): ok: [Master] => {
null_resource.ansible (local-exec):     "k8sjc.stdout": "kubeadm join 192.168.1.100:6443 --token ey84ma.un5i6jiis58yxlei --discovery-token-ca-cert-hash sha256:02ccc0fa196b97fbafc8f85dfb9c3297441c6260ab2262384aeffe60e8e5f879 "
null_resource.ansible (local-exec): }

null_resource.ansible (local-exec): TASK [kubemasters : Download kube-flannel manifest] ****************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [19m29s elapsed]
null_resource.ansible: Still creating... [19m39s elapsed]
null_resource.ansible: Still creating... [19m49s elapsed]

null_resource.ansible (local-exec): TASK [kubemasters : Apply flannel manifest to the cluster] *********************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Apply RBAC] ************************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [19m59s elapsed]

null_resource.ansible (local-exec): TASK [kubemasters : Remove kube-flannel.yml manifest] **************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [20m9s elapsed]

null_resource.ansible (local-exec): TASK [kubemasters : Remove RBAC] ***********************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Get clusterrolebinding] ************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Create clusterrolebinding] *********************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Get kubernetes node list] **********************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : Register node list] ****************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : include_tasks] *********************************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): TASK [kubemasters : include_tasks] *********************************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): PLAY [kubenodes] ***************************************************************

null_resource.ansible (local-exec): TASK [kubenodes : include_tasks] ***********************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/kubenodes/tasks/create.yml for Node01
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/kubenodes/tasks/create.yml for Node02

null_resource.ansible (local-exec): TASK [kubenodes : Debug ansible_host] ******************************************
null_resource.ansible (local-exec): ok: [Node01] => {
null_resource.ansible (local-exec):     "inventory_hostname": "Node01"
null_resource.ansible (local-exec): }

null_resource.ansible (local-exec): TASK [kubenodes : Debug ansible_host] ******************************************
null_resource.ansible (local-exec): ok: [Node02] => {
null_resource.ansible (local-exec):     "inventory_hostname": "Node02"
null_resource.ansible (local-exec): }
null_resource.ansible: Still creating... [20m19s elapsed]
null_resource.ansible: Still creating... [20m29s elapsed]
null_resource.ansible (local-exec): ASYNC POLL on Node02: jid=821377840015.14531 started=1 finished=0
null_resource.ansible (local-exec): ASYNC POLL on Node01: jid=718602601495.18913 started=1 finished=0

null_resource.ansible (local-exec): TASK [kubenodes : Join Cluster] ************************************************
null_resource.ansible (local-exec): changed: [Node02]

null_resource.ansible (local-exec): TASK [kubenodes : Create local file system where to store mysql databases] *****
null_resource.ansible (local-exec): skipping: [Node02]

null_resource.ansible (local-exec): TASK [kubenodes : Join Cluster] ************************************************
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [kubenodes : include_tasks] ***********************************************
null_resource.ansible (local-exec): skipping: [Node02]
null_resource.ansible: Still creating... [20m39s elapsed]
null_resource.ansible: Still creating... [20m49s elapsed]

null_resource.ansible (local-exec): TASK [kubenodes : Create local file system where to store mysql databases] *****
null_resource.ansible (local-exec): changed: [Node01]

null_resource.ansible (local-exec): TASK [kubenodes : include_tasks] ***********************************************
null_resource.ansible (local-exec): skipping: [Node01]

null_resource.ansible (local-exec): PLAY [deployer] ****************************************************************

null_resource.ansible (local-exec): TASK [Gathering Facts] *********************************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [ingress-nginx-controller : include_tasks] ********************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/ingress-nginx-controller/tasks/create.yml for Master

null_resource.ansible (local-exec): TASK [ingress-nginx-controller : Add ingress-nginx repository] *****************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [20m59s elapsed]
null_resource.ansible: Still creating... [21m9s elapsed]
null_resource.ansible: Still creating... [21m19s elapsed]
null_resource.ansible: Still creating... [21m29s elapsed]
null_resource.ansible: Still creating... [21m39s elapsed]
null_resource.ansible: Still creating... [21m49s elapsed]
null_resource.ansible: Still creating... [21m59s elapsed]
null_resource.ansible: Still creating... [22m9s elapsed]
null_resource.ansible: Still creating... [22m19s elapsed]
null_resource.ansible: Still creating... [22m29s elapsed]
null_resource.ansible: Still creating... [22m39s elapsed]
null_resource.ansible: Still creating... [22m49s elapsed]
null_resource.ansible: Still creating... [22m59s elapsed]

null_resource.ansible (local-exec): TASK [ingress-nginx-controller : Install ingress-nginx] ************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ingress-nginx-controller : Patch nginx-service] **************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ingress-nginx-controller : include_tasks] ********************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): TASK [cert-manager : include_tasks] ********************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/cert-manager/tasks/create.yml for Master
null_resource.ansible: Still creating... [23m9s elapsed]

null_resource.ansible (local-exec): TASK [cert-manager : Download cert-manager manifest to the cluster.] ***********
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [23m19s elapsed]
null_resource.ansible: Still creating... [23m29s elapsed]

null_resource.ansible (local-exec): TASK [cert-manager : Apply cert-manager manifest to the cluster.] **************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [cert-manager : Remove cert-manager manifest from the cluster.] ***********
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [cert-manager : include_tasks] ********************************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): TASK [mysql : include_tasks] ***************************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/mysql/tasks/create.yml for Master
null_resource.ansible: Still creating... [23m39s elapsed]

null_resource.ansible (local-exec): TASK [mysql : Label node] ******************************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [mysql : Create a mysql namespace] ****************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [mysql : Get mysql secret] ************************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [mysql : Register mysql admin password] ***********************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible: Still creating... [23m49s elapsed]

null_resource.ansible (local-exec): TASK [mysql : Deploy MySQL secrets] ********************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [mysql : Create MySQL configmap for adding users] *************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [mysql : Create MySQL service] ********************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [23m59s elapsed]
null_resource.ansible: Still creating... [24m9s elapsed]
null_resource.ansible: Still creating... [24m19s elapsed]
null_resource.ansible: Still creating... [24m29s elapsed]
null_resource.ansible: Still creating... [24m39s elapsed]
null_resource.ansible: Still creating... [24m49s elapsed]
null_resource.ansible: Still creating... [24m59s elapsed]
null_resource.ansible: Still creating... [25m9s elapsed]
null_resource.ansible: Still creating... [25m19s elapsed]
null_resource.ansible: Still creating... [25m29s elapsed]
null_resource.ansible: Still creating... [25m39s elapsed]
null_resource.ansible: Still creating... [25m49s elapsed]
null_resource.ansible: Still creating... [25m59s elapsed]
null_resource.ansible: Still creating... [26m9s elapsed]

null_resource.ansible (local-exec): TASK [mysql : Create MySQL deployment] *****************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [mysql : include_tasks] ***************************************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): TASK [ghost : include_tasks] ***************************************************
null_resource.ansible (local-exec): included: /SAFE_VOLUME/infra/ansible/roles/ghost/tasks/create.yml for Master

null_resource.ansible (local-exec): TASK [ghost : Create a ghost namespace] ****************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : Get ghost user secret] *******************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [26m19s elapsed]

null_resource.ansible (local-exec): TASK [ghost : Get ghost password secret] ***************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : Get ghost database secret] ***************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : Register ghost username] *****************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [ghost : Register ghost user password] ************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [ghost : Register ghost database] *****************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [ghost : Look for MySQL pod name] *****************************************
null_resource.ansible (local-exec): ok: [Master]

null_resource.ansible (local-exec): TASK [ghost : Register Pod name] ***********************************************
null_resource.ansible (local-exec): ok: [Master]
null_resource.ansible: Still creating... [26m29s elapsed]

null_resource.ansible (local-exec): TASK [ghost : Create Ghost mysql user/database/credentials] ********************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : Debug user creation] *********************************************
null_resource.ansible (local-exec): ok: [Master] => {
null_resource.ansible (local-exec):     "createuser": {
null_resource.ansible (local-exec):         "changed": true,
null_resource.ansible (local-exec):         "failed": false,
null_resource.ansible (local-exec):         "return_code": 0,
null_resource.ansible (local-exec):         "stderr": "mysql: [Warning] Using a password on the command line interface can be insecure.\nmysql: [Warning] Using a password on the command line interface can be insecure.\nmysql: [Warning] Using a password on the command line interface can be insecure.\n",
null_resource.ansible (local-exec):         "stderr_lines": [
null_resource.ansible (local-exec):             "mysql: [Warning] Using a password on the command line interface can be insecure.",
null_resource.ansible (local-exec):             "mysql: [Warning] Using a password on the command line interface can be insecure.",
null_resource.ansible (local-exec):             "mysql: [Warning] Using a password on the command line interface can be insecure."
null_resource.ansible (local-exec):         ],
null_resource.ansible (local-exec):         "stdout": "Creating user 40FAhh8s\nCreating DB ghost\nGranting permissions to user 40FAhh8s to database: ghost\n",
null_resource.ansible (local-exec):         "stdout_lines": [
null_resource.ansible (local-exec):             "Creating user 40FAhh8s",
null_resource.ansible (local-exec):             "Creating DB ghost",
null_resource.ansible (local-exec):             "Granting permissions to user 40FAhh8s to database: ghost"
null_resource.ansible (local-exec):         ]
null_resource.ansible (local-exec):     }
null_resource.ansible (local-exec): }

null_resource.ansible (local-exec): TASK [ghost : Create tls issuer] ***********************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : Deploy Ghost secrets] ********************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [26m39s elapsed]

null_resource.ansible (local-exec): TASK [ghost : Create Ghost service] ********************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : Create Ghost configmap] ******************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [26m49s elapsed]
null_resource.ansible: Still creating... [26m59s elapsed]
null_resource.ansible: Still creating... [27m9s elapsed]
null_resource.ansible: Still creating... [27m19s elapsed]
null_resource.ansible: Still creating... [27m29s elapsed]
null_resource.ansible: Still creating... [27m39s elapsed]
null_resource.ansible: Still creating... [27m49s elapsed]
null_resource.ansible: Still creating... [27m59s elapsed]
null_resource.ansible: Still creating... [28m9s elapsed]
null_resource.ansible: Still creating... [28m19s elapsed]
null_resource.ansible: Still creating... [28m29s elapsed]
null_resource.ansible: Still creating... [28m39s elapsed]
null_resource.ansible: Still creating... [28m49s elapsed]
null_resource.ansible: Still creating... [28m59s elapsed]
null_resource.ansible: Still creating... [29m9s elapsed]
null_resource.ansible: Still creating... [29m19s elapsed]
null_resource.ansible: Still creating... [29m29s elapsed]
null_resource.ansible: Still creating... [29m39s elapsed]
null_resource.ansible: Still creating... [29m49s elapsed]
null_resource.ansible: Still creating... [29m59s elapsed]
null_resource.ansible: Still creating... [30m9s elapsed]
null_resource.ansible: Still creating... [30m19s elapsed]
null_resource.ansible: Still creating... [30m29s elapsed]
null_resource.ansible: Still creating... [30m39s elapsed]
null_resource.ansible: Still creating... [30m49s elapsed]

null_resource.ansible (local-exec): TASK [ghost : Create Ghost deployment] *****************************************
null_resource.ansible (local-exec): changed: [Master]
null_resource.ansible: Still creating... [30m59s elapsed]

null_resource.ansible (local-exec): TASK [ghost : Create Ghost ingress] ********************************************
null_resource.ansible (local-exec): changed: [Master]

null_resource.ansible (local-exec): TASK [ghost : include_tasks] ***************************************************
null_resource.ansible (local-exec): skipping: [Master]

null_resource.ansible (local-exec): PLAY RECAP *********************************************************************
null_resource.ansible (local-exec): Master                     : ok=94   changed=65   unreachable=0    failed=0    skipped=10   rescued=0    ignored=1
null_resource.ansible (local-exec): Node01                     : ok=49   changed=34   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0
null_resource.ansible (local-exec): Node02                     : ok=32   changed=20   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0

null_resource.ansible: Creation complete after 31m2s [id=6460797351548999677]

Apply complete! Resources: 37 added, 0 changed, 0 destroyed.

Outputs:

WARNING = ""
ansible_exec_command = "ansible-playbook -e jump_host='20.86.240.158' -e admin_user='azureuser' -e subnet_cidr_private='192.168.1.0/24' -e private_lan_master='192.168.1.100' -e private_lan_node01='192.168.1.101' -e private_lan_node02='192.168.1.102' -e internal_private_key_file='resources/internal.pem' -e external_private_key_file='resources/external.pem' -e ghost_url='ghosthope.westeurope.cloudapp.azure.com' -e prefix='mfvilla' ../ansible/playbooks/create.yml"
ghost_http_service_url = "http://ghosthope.westeurope.cloudapp.azure.com"
ghost_https_service_url = "https://ghosthope.westeurope.cloudapp.azure.com"
master_private_address = "192.168.1.100"
master_public_address = "20.86.240.158"
node01_private_address = "192.168.1.101"
node02_private_address = "192.168.1.102"
ssh_admin_user = "azureuser"
ssh_master_connection = "ssh -i resources/external.pem azureuser@20.86.240.158"
ssh_node01_connection = "ssh -i resources/internal.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand='ssh -i resources/external.pem -W %h:%p -q azureuser@20.86.240.158' azureuser@192.168.1.101"
ssh_node02_connection = "ssh -i resources/internal.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand='ssh -i resources/external.pem -W %h:%p -q azureuser@20.86.240.158' azureuser@192.168.1.102"
subnet_cidr_private = tolist([
  "192.168.1.0/24",
])
virtual_network_cidr = tolist([
  "192.168.0.0/16",
])
